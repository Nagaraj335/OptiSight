{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3288c5a1",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision numpy matplotlib seaborn scikit-learn Pillow tqdm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1436375",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1285a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision import transforms, models\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b3794",
   "metadata": {},
   "source": [
    "## Step 3: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a023eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset path\n",
    "DATASET_ROOT = r\"D:\\AMDNet23 Fundus Image Dataset for  Age-Related Macular Degeneration Disease Detection\\AMDNet23 Fundus Image Dataset for  Age-Related Macular Degeneration Disease Detection\\AMDNet23 Dataset\"\n",
    "TRAIN_DIR = os.path.join(DATASET_ROOT, \"train\")\n",
    "VALID_DIR = os.path.join(DATASET_ROOT, \"valid\")\n",
    "\n",
    "# Hyperparameters\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "NUM_CLASSES = 4\n",
    "CLASS_NAMES = ['amd', 'cataract', 'diabetes', 'normal']\n",
    "\n",
    "print(f\"Train directory: {TRAIN_DIR}\")\n",
    "print(f\"Valid directory: {VALID_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54ac201",
   "metadata": {},
   "source": [
    "## Step 4: Create Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731247ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMDDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "        self.class_to_idx = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
    "        \n",
    "        for class_name in CLASS_NAMES:\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            if os.path.exists(class_dir):\n",
    "                for img_name in os.listdir(class_dir):\n",
    "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "                        self.images.append(os.path.join(class_dir, img_name))\n",
    "                        self.labels.append(self.class_to_idx[class_name])\n",
    "        \n",
    "        print(f\"Loaded {len(self.images)} images from {root_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.images[idx]).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec498c59",
   "metadata": {},
   "source": [
    "## Step 5: Define Transforms and Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fd2882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Validation transforms (no augmentation)\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = AMDDataset(TRAIN_DIR, transform=train_transform)\n",
    "val_dataset = AMDDataset(VALID_DIR, transform=val_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nTraining samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93278a",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Denormalize for visualization\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(images):\n",
    "        img = denormalize(images[i]).permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(CLASS_NAMES[labels[i]])\n",
    "        ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbea111f",
   "metadata": {},
   "source": [
    "## Step 7: Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57839a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained ResNet50\n",
    "model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Modify the final layer for our classes\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Sequential(\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(num_features, 512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.Linear(512, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Print model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9066a6a6",
   "metadata": {},
   "source": [
    "## Step 8: Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e52dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84fa97",
   "metadata": {},
   "source": [
    "## Step 9: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17170ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Training\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return running_loss / total, correct / total\n",
    "\n",
    "def validate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(loader, desc=\"Validating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return running_loss / total, correct / total, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfcc37b",
   "metadata": {},
   "source": [
    "## Step 10: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2da49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training history\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "best_acc = 0.0\n",
    "best_model_wts = None\n",
    "\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print(f\"✓ New best model saved! Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"Training complete! Best validation accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff051b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from saved model\n",
    "checkpoint = torch.load('outputs/models/amd_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "best_acc = checkpoint['val_acc']\n",
    "best_model_wts = model.state_dict()\n",
    "\n",
    "# Initialize history for additional epochs\n",
    "history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "ADDITIONAL_EPOCHS = 30  # Train for 30 more epochs\n",
    "\n",
    "print(f\"Continuing training from validation accuracy: {best_acc:.4f}\")\n",
    "print(f\"Training for {ADDITIONAL_EPOCHS} more epochs...\\n\")\n",
    "\n",
    "for epoch in range(ADDITIONAL_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{ADDITIONAL_EPOCHS}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc, _, _ = validate(model, val_loader, criterion)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'class_names': CLASS_NAMES,\n",
    "            'val_acc': best_acc\n",
    "        }, 'outputs/models/amd_model.pth')\n",
    "        print(f\"✓ New best model saved! Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"Continued training complete! Best validation accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24985720",
   "metadata": {},
   "source": [
    "## Step 10B: Continue Training (Optional)\n",
    "\n",
    "**Run this to continue training from a saved model for more epochs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7aa8e8",
   "metadata": {},
   "source": [
    "## Step 11: Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a48a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "ax1.plot(history['train_loss'], label='Train Loss', linewidth=2)\n",
    "ax1.plot(history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy\n",
    "ax2.plot(history['train_acc'], label='Train Acc', linewidth=2)\n",
    "ax2.plot(history['val_acc'], label='Val Acc', linewidth=2)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec8847b",
   "metadata": {},
   "source": [
    "## Step 12: Evaluate Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25486901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "# Final evaluation\n",
    "val_loss, val_acc, all_preds, all_labels = validate(model, val_loader, criterion)\n",
    "\n",
    "print(f\"\\nFinal Validation Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Final Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=CLASS_NAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb24658",
   "metadata": {},
   "source": [
    "## Step 13: Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ffd20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c623cdc9",
   "metadata": {},
   "source": [
    "## Step 14: Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19192fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "os.makedirs('outputs/models', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'class_names': CLASS_NAMES,\n",
    "    'val_acc': best_acc\n",
    "}, 'outputs/models/amd_model.pth')\n",
    "\n",
    "print(\"Model saved to outputs/models/amd_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model (skip Step 10 if using this)\n",
    "checkpoint = torch.load('outputs/models/amd_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "best_acc = checkpoint['val_acc']\n",
    "best_model_wts = model.state_dict()\n",
    "\n",
    "print(f\"✓ Model loaded successfully!\")\n",
    "print(f\"Validation accuracy: {best_acc:.4f}\")\n",
    "print(\"\\nYou can now skip to Step 12 for evaluation or Step 15 for predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b5cd9",
   "metadata": {},
   "source": [
    "## Alternative: Load Previously Trained Model\n",
    "\n",
    "**Run this cell instead of Step 10 if you already have a trained model saved**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214e5db5",
   "metadata": {},
   "source": [
    "## Step 15: Test Prediction on Single Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2d2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(image_path):\n",
    "    \"\"\"Predict class for a single image\"\"\"\n",
    "    # Load and transform image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = val_transform(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Predict\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        conf, pred = probs.max(1)\n",
    "    \n",
    "    predicted_class = CLASS_NAMES[pred.item()]\n",
    "    confidence = conf.item() * 100\n",
    "    \n",
    "    # Display\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Prediction: {predicted_class.upper()}\\nConfidence: {confidence:.2f}%\", fontsize=14)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Show all probabilities\n",
    "    print(\"\\nClass Probabilities:\")\n",
    "    for i, name in enumerate(CLASS_NAMES):\n",
    "        print(f\"  {name}: {probs[0][i].item()*100:.2f}%\")\n",
    "    \n",
    "    return predicted_class, confidence\n",
    "\n",
    "# Example usage (uncomment and modify path):\n",
    "# predict_image(r\"path\\to\\your\\image.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99c1f8d",
   "metadata": {},
   "source": [
    "## Step 16: Visualize Predictions on Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005ae4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some validation images\n",
    "model.eval()\n",
    "images, labels = next(iter(val_loader))\n",
    "images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, preds = outputs.max(1)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(3, 4, figsize=(14, 10))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(images):\n",
    "        img = denormalize(images[i].cpu()).permute(1, 2, 0).numpy()\n",
    "        img = np.clip(img, 0, 1)\n",
    "        ax.imshow(img)\n",
    "        \n",
    "        true_label = CLASS_NAMES[labels[i]]\n",
    "        pred_label = CLASS_NAMES[preds[i]]\n",
    "        color = 'green' if labels[i] == preds[i] else 'red'\n",
    "        ax.set_title(f\"True: {true_label}\\nPred: {pred_label}\", color=color, fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
